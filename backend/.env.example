# Prism Backend Environment Variables (example)
# Copy to .env and fill in your own values. NEVER commit real secrets.

# LLM mode: local (default) or api
LLM_MODE=local

# Provider when LLM_MODE=api (openai | google)
LLM_PROVIDER=openai

# API key for provider (set your own; do not commit actual key)
LLM_API_KEY=YOUR_API_KEY_HERE

# Vector database configuration (Qdrant)
VECTOR_DB=qdrant
QDRANT_URL=http://qdrant:6333
# If using Qdrant Cloud, set QDRANT_API_KEY instead of internal URL
# QDRANT_API_KEY=YOUR_QDRANT_API_KEY
QDRANT_COLLECTION=prism_chunks
EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
# Enable embedded (in-process) Qdrant instead of a separate Docker service. Uses local storage path.
QDRANT_EMBEDDED=0
# Storage path for embedded mode (auto-created). Ignored when not embedded.
QDRANT_STORAGE=./data/qdrant_storage

# Frontend origin for CORS (deployment: set to your Vercel domain, dev: http://localhost:5173 )
FRONTEND_ORIGIN=http://localhost:5173
# Additional comma-separated origins (optional)
# CORS_ADDITIONAL_ORIGINS=https://your-secondary-origin.com,https://another.example

# Remote model name (varies per provider)
LLM_MODEL_NAME=gpt-4o-mini
# For Google Gemini you might use: gemini-1.5-flash or gemini-1.5-pro

# Base URL overrides (optional)
# LLM_API_BASE=https://api.openai.com/v1

# Local model path override (when LLM_MODE=local)
# LLM_MODEL_PATH=C:\\models\\llm\\mistral-7b-instruct-v0.2.Q4_K_M.gguf

# Threads for local llama.cpp (optional)
LLM_THREADS=8

# Additional future vars (placeholders)
# LLM_TIMEOUT=60
# LLM_MAX_TOKENS=512
