# Prism Backend Environment Variables
# Copy this file to `.env` and fill in real values. NEVER commit the actual `.env`.

# LLM operating mode: `local` (use GGUF model) or `api` (use remote provider via HTTP)
LLM_MODE=api

# Provider identifier: `gemini` | `openai` | (future: `groq`, `anthropic`, etc.)
LLM_PROVIDER=gemini

# API key for the chosen provider (keep secret)
LLM_API_KEY=AIzaSyBd1vZcX7YuG96LxFZPlqvJF_vn1WN8uvo

# Remote model name (Gemini example). For OpenAI style endpoints use e.g. `gpt-4o-mini`.
LLM_MODEL_NAME=gemini-2.5-flash

# Optional custom base URL for OpenAI-compatible endpoints. Leave blank for Gemini.
# LLM_API_BASE=https://api.openai.com/v1

# Local model path (only when LLM_MODE=local). Example:
# LLM_MODEL_PATH=models/llm/mistral-7b-instruct-v0.2.Q4_K_M.gguf

# GPU acceleration (only affects local mode). Set GPU layers to >0 to enable.
# LLM_GPU_LAYERS=35
# LLM_CTX_SIZE=4096

# Logging level (info|debug|warning|error)
# LOG_LEVEL=info

# Additional optional tuning variables (uncomment as needed):
# LLM_MAX_TOKENS=512
# LLM_TEMPERATURE=0.7
# LLM_TOP_P=0.9

# NOTE:
# - Keep `.env` out of version control (already covered in .gitignore).
# - Rotate API keys periodically.
# - For production, prefer a secrets manager over plain .env files.# Prism Backend Environment Variables (example)
# Copy to .env and fill in your own values. NEVER commit real secrets.

# LLM mode: local (default) or api
LLM_MODE=local

# Provider when LLM_MODE=api (openai | google)
LLM_PROVIDER=openai

# API key for provider (set your own; do not commit actual key)
LLM_API_KEY=AIzaSyBd1vZcX7YuG96LxFZPlqvJF_vn1WN8uvo

# Remote model name (varies per provider)
LLM_MODEL_NAME=gpt-4o-mini
# For Google Gemini you might use: gemini-1.5-flash or gemini-1.5-pro

# Base URL overrides (optional)
# LLM_API_BASE=https://api.openai.com/v1

# Local model path override (when LLM_MODE=local)
# LLM_MODEL_PATH=C:\\models\\llm\\mistral-7b-instruct-v0.2.Q4_K_M.gguf

# Threads for local llama.cpp (optional)
LLM_THREADS=8

# Additional future vars (placeholders)
# LLM_TIMEOUT=60
# LLM_MAX_TOKENS=512
